{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyP0NHaZs4bSGYrf5MuY5b9I"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Azk5-0CI2J0y","executionInfo":{"status":"ok","timestamp":1743910175837,"user_tz":300,"elapsed":329861,"user":{"displayName":"Mohan Avinash Kakulavarapu","userId":"02418669374113699307"}},"outputId":"b68458ea-73e5-45cf-d2b2-8f19942c88f9"},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/20\n","\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 73ms/step - loss: 2.9280\n","Epoch 2/20\n","\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 72ms/step - loss: 1.8955\n","Epoch 3/20\n","\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 71ms/step - loss: 1.6239\n","Epoch 4/20\n","\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 71ms/step - loss: 1.4832\n","Epoch 5/20\n","\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 70ms/step - loss: 1.3933\n","Epoch 6/20\n","\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 72ms/step - loss: 1.3330\n","Epoch 7/20\n","\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 74ms/step - loss: 1.2880\n","Epoch 8/20\n","\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 72ms/step - loss: 1.2428\n","Epoch 9/20\n","\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 71ms/step - loss: 1.2057\n","Epoch 10/20\n","\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 70ms/step - loss: 1.1666\n","Epoch 11/20\n","\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 71ms/step - loss: 1.1295\n","Epoch 12/20\n","\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 71ms/step - loss: 1.0856\n","Epoch 13/20\n","\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 71ms/step - loss: 1.0441\n","Epoch 14/20\n","\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 71ms/step - loss: 1.0039\n","Epoch 15/20\n","\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 72ms/step - loss: 0.9565\n","Epoch 16/20\n","\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 71ms/step - loss: 0.9082\n","Epoch 17/20\n","\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 72ms/step - loss: 0.8570\n","Epoch 18/20\n","\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 72ms/step - loss: 0.8054\n","Epoch 19/20\n","\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 71ms/step - loss: 0.7558\n","Epoch 20/20\n","\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 71ms/step - loss: 0.7038\n","ROMEO: I herere owathe wn t se thout,\n","Tind w her's lllir, the mouthe ar the thand the, aven bes west be the whe t are ther anoutht s me ar me hen thinde an thes t ange my me the f t myor thon ath non wh the ho thon s at me ther s myour th whe win the we gere s he ce tharathouthe tis m the be me be thes t bl and t nge we! banome me a t omanor mas as s se houchaloun hea thenche the he ak t mal wighe we whe s y w anouth the thearis the the the wo s win hethe harthin thar bl he o an areathallllle, llo ithed an ouprs are te he win wen the t ad be s my wa whend he che wis h be to ithore he athe s the ithe IO:\n","\n","Hanor thangh athong wis he hang merereandyowhe the ant the te the he anoure my t thenishe wan ano wng the ase hang ar pore me ithe to ous isthe athe me athathe t co a ur llllle t d athe me touthtousean m se ate s d ther t, the thourst s stod thour me the the g t in thicond thinoour anthe al s s angrere whenot l an thend bed mor t winghe the the ate the weathe is we, m t the ou sthe th ang the\n"]}],"source":["import numpy as np\n","import tensorflow as tf\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import LSTM, Dense, Embedding\n","from tensorflow.keras.optimizers import Adam\n","\n","# Load text dataset (e.g., Shakespeare Sonnets)\n","path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')\n","text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n","\n","# Preprocessing\n","vocab = sorted(set(text))\n","char2idx = {u: i for i, u in enumerate(vocab)}\n","idx2char = np.array(vocab)\n","text_as_int = np.array([char2idx[c] for c in text])\n","\n","# Create training examples and targets\n","seq_length = 100\n","examples_per_epoch = len(text)//(seq_length + 1)\n","\n","char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n","sequences = char_dataset.batch(seq_length+1, drop_remainder=True)\n","\n","def split_input_target(chunk):\n","    input_text = chunk[:-1]\n","    target_text = chunk[1:]\n","    return input_text, target_text\n","\n","dataset = sequences.map(split_input_target)\n","\n","# Batch size\n","BATCH_SIZE = 64\n","BUFFER_SIZE = 10000\n","\n","dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n","\n","# Model definition\n","vocab_size = len(vocab)\n","embedding_dim = 256\n","rnn_units = 1024\n","\n","model = Sequential([\n","    Embedding(vocab_size, embedding_dim, input_length=None),\n","    LSTM(rnn_units, return_sequences=True, stateful=False, recurrent_initializer='glorot_uniform'),\n","    Dense(vocab_size)\n","])\n","\n","# Compile model\n","model.compile(optimizer=Adam(), loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True))\n","\n","# Training the model\n","EPOCHS = 20\n","history = model.fit(dataset, epochs=EPOCHS)\n","\n","# Text generation\n","def generate_text(model, start_string, temperature=1.0):\n","    input_eval = [char2idx[s] for s in start_string]\n","    input_eval = tf.expand_dims(input_eval, 0)\n","    text_generated = []\n","\n","    for i in range(1000):\n","        predictions = model(input_eval)\n","        predictions = tf.squeeze(predictions, 0)\n","        predictions = predictions / temperature\n","        predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n","\n","        input_eval = tf.expand_dims([predicted_id], 0)\n","        text_generated.append(idx2char[predicted_id])\n","\n","    return start_string + ''.join(text_generated)\n","\n","# Generate new text\n","print(generate_text(model, start_string=\"ROMEO: \", temperature=0.5))\n"]},{"cell_type":"code","source":[],"metadata":{"id":"Dak3N1XyGgv0"},"execution_count":null,"outputs":[]}]}